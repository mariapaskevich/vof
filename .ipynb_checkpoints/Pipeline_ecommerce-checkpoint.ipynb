{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vof\n",
    "from vof import forecaster\n",
    "from vof import optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from darts.utils.statistics import check_seasonality, plot_acf, plot_residuals_analysis\n",
    "import statsmodels.tsa.stattools\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import time\n",
    "import random\n",
    "#from fbprophet import Prophet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import lightgbm as lgb\n",
    "from datetime import date\n",
    "import nevergrad as ng\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "    \n",
    "from darts.utils import timeseries_generation as tg\n",
    "from darts import TimeSeries\n",
    "from darts.metrics import rmse\n",
    "# from darts import SeasonalityMode, TrendMode, ModelMode\n",
    "\n",
    "from darts.models import Theta, FFT, ExponentialSmoothing, Prophet, RegressionModel,NaiveSeasonal,LightGBMModel,RandomForest,forecasting\n",
    "# import darts.models\n",
    "from darts.utils.utils import SeasonalityMode, TrendMode, ModelMode\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [18, 8]\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data = pd.read_csv('data/ecomm/sales_train_data_merged.csv',index_col=0).fillna(0)\n",
    "data\n",
    "\n",
    "top_items = data.groupby('item_id').sum().sort_values(by='sales').index[-100:]\n",
    "data.loc[data.item_id.isin(top_items)].to_csv('data/ecomm/sales_train_data_merged_top100_items.csv')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/ecomm/sales_train_data_merged_top100_items.csv',index_col=0).fillna(0)\n",
    "data.index = data.index.astype('datetime64[ns]')\n",
    "\n",
    "#data['sin_wday'] = np.sin(2*np.pi*data.wday/7)\n",
    "#data['cos_wday'] = np.cos(2*np.pi*data.wday/7)\n",
    "\n",
    "#data.set_index('date', inplace=True)\n",
    "data_subset = data.loc['2011-10-01':'2012-10-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_items = data_subset.groupby('item_id').sum().sort_values(by='sales').index[-5:]\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "alt.Chart(data_subset.loc[data_subset.item_id.isin(top_items)].reset_index()).mark_line().encode(\n",
    "    x='date:T',\n",
    "    y=alt.Y('sales:Q'),\n",
    "    color='item_id',\n",
    "    tooltip=['item_id','date:T','sales:Q']\n",
    ").properties(width=500, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_item(item_id='FOODS_3_555'):\n",
    "    item = data_subset.loc[data_subset.item_id.isin([item_id])]\n",
    "    item.loc[item.sales<10,'sales'] = item.sales.median()\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, \n",
    "               input_timeseries,\n",
    "               past_covariates=None, \n",
    "               future_covariates=None, \n",
    "               retrain = False, \n",
    "               plot_backtest=True):\n",
    "    # Past and future covariates are optional because they won't always be used in our tests\n",
    "    \n",
    "    # We backtest the model on the last 20% of the flow series, with a horizon of 10 steps:\n",
    "    backtest = model.historical_forecasts(series=input_timeseries, \n",
    "                                          past_covariates=past_covariates,\n",
    "                                          future_covariates=future_covariates,\n",
    "                                          start=0.33, \n",
    "                                          retrain=retrain,\n",
    "                                          verbose=True, \n",
    "                                          forecast_horizon=7)\n",
    "    \n",
    "    if plot_backtest:\n",
    "        input_timeseries[-len(backtest)-14:].plot()\n",
    "        backtest.plot(label='backtest (n=10)')\n",
    "    print(str(model)+' Backtest RMSE = {}'.format(rmse(input_timeseries, backtest)))\n",
    "\n",
    "\n",
    "def get_historical_forecasts(model, \n",
    "                             input_timeseries,\n",
    "                             future_covariates,\n",
    "                             model_display_name, \n",
    "                             future_covs=False):\n",
    "\n",
    "    horizon = 7\n",
    "    \n",
    "    if future_covs == False:\n",
    "        #print(model_display_name, 'no future_covs')\n",
    "        forecast = model.historical_forecasts(input_timeseries,\n",
    "                                              forecast_horizon=horizon, \n",
    "                                              stride=1, \n",
    "                                              verbose=True,\n",
    "                                              start=60, \n",
    "                                              last_points_only=False, \n",
    "                                              overlap_end=False)\n",
    "    else:\n",
    "        #print(model_display_name, 'with future_covs')\n",
    "        forecast = model.historical_forecasts(input_timeseries,\n",
    "                                              forecast_horizon=horizon, \n",
    "                                              stride=1, \n",
    "                                              verbose=True,\n",
    "                                              future_covariates=future_covariates,\n",
    "                                              start=60,\n",
    "                                              last_points_only=False,\n",
    "                                              overlap_end=False)\n",
    "        \n",
    "    res_df = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(forecast)):\n",
    "        pred_df = forecast[i].pd_dataframe()\n",
    "        pred_df['prediction_date'] = forecast[i].get_timestamp_at_point(0)\n",
    "        pred_df['model'] = model_display_name\n",
    "        pred_df.reset_index(inplace=True)\n",
    "        pred_df.columns = ['ts','value','prediction_date','model']\n",
    "        res_df = res_df.append(pred_df)\n",
    "    \n",
    "    if future_covs == False:\n",
    "        return res_df.iloc[:-(horizon*7)]\n",
    "    else:\n",
    "        return res_df\n",
    "    #return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_forecast_for_item(itemid):\n",
    "    \n",
    "    input_timeseries = TimeSeries.from_dataframe(get_input_item(itemid), value_cols=['sales'])\n",
    "\n",
    "    future = TimeSeries.from_dataframe(get_input_item(itemid), \n",
    "                                       value_cols=['sin_wday','cos_wday','special_events'])\n",
    "    \n",
    "    print(future)\n",
    "    # We first set aside the first 33% as training series:\n",
    "    input_train, _ = input_timeseries.split_before(0.33)\n",
    "\n",
    "    #naive_model_daily = NaiveSeasonal(K=1)\n",
    "    #naive_model_daily.fit(input_train)\n",
    "\n",
    "    naive_model_weekly = NaiveSeasonal(K=7)\n",
    "    naive_model_weekly.fit(input_train)\n",
    "\n",
    "    regr_model_lags =  RegressionModel(lags=list(range(-14,0)))\n",
    "    regr_model_lags.fit(input_train)\n",
    "\n",
    "    regr_model_cov = RegressionModel(lags=list(range(-14,0)),lags_future_covariates=[-14,-7,0,1,7])\n",
    "    regr_model_cov.fit(input_train,future_covariates=future)\n",
    "\n",
    "    lgbm_model_cov = LightGBMModel(lags=list(range(-14,0)),lags_future_covariates=[-14,-7,0,1,7])\n",
    "    lgbm_model_cov.fit(input_train,future_covariates=future)\n",
    "\n",
    "    lgbm_model = LightGBMModel(lags=list(range(-14,0)))\n",
    "    lgbm_model.fit(input_train)\n",
    "    \n",
    "    theta_model =  Theta(2, seasonality_period=7, season_mode=SeasonalityMode.MULTIPLICATIVE)\n",
    "    theta_model.fit(input_train)\n",
    "\n",
    "\n",
    "\n",
    "    #eval_model(naive_model_daily, input_timeseries, retrain=True, plot_backtest=False)\n",
    "    #eval_model(naive_model_weekly,input_timeseries, retrain=True, plot_backtest=False)\n",
    "    #eval_model(regr_model_lags,input_timeseries, plot_backtest=False)\n",
    "    #eval_model(regr_model_cov,input_timeseries, plot_backtest=False)\n",
    "    #eval_model(lgbm_model_cov,input_timeseries, plot_backtest=False)\n",
    "    \n",
    "    models = {'Naive weekly':[naive_model_weekly,False],\n",
    "              'RegressionModelLags':[regr_model_lags,False],\n",
    "              'RegressionModelCov':[regr_model_cov,True],\n",
    "              #'LGBM_cov':[lgbm_model_cov,True],\n",
    "              #'LGBM_no_cov':[lgbm_model,False],\n",
    "              #'Theta':[theta_model,False],\n",
    "              #'FFT':[fft_model,False],\n",
    "              #'ExponentialSmoothing':[exp_smooth_model,False],\n",
    "              #'Prophet':[prophet_model,False]\n",
    "         }\n",
    "\n",
    "    forcasts_df = pd.DataFrame()\n",
    "\n",
    "    runtime_df = pd.DataFrame(index=models.keys(),columns=['runtime'])\n",
    "    \n",
    "    parallel_forecasts = (Parallel(n_jobs=-1, verbose = 1000)\\\n",
    "                          (delayed(get_historical_forecasts)(models[model][0],\n",
    "                                                             input_timeseries=input_timeseries,\n",
    "                                                             model_display_name = model,\n",
    "                                                             future_covariates=future,\n",
    "                                                             future_covs = models[model][1]) for model in models.keys()))\n",
    "    #merge all models into a single dataframe forcasts_df\n",
    "    for i in parallel_forecasts:\n",
    "        forcasts_df = forcasts_df.append(i)\n",
    "\n",
    "    forcasts_df['item_id'] = itemid\n",
    "\n",
    "    return forcasts_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_df_parallel = (Parallel(n_jobs=-1, verbose = 1000)\\\n",
    "                            (delayed(create_forecast_for_item)(item) for item in top_items.values))\n",
    "\n",
    "#merge all items into a single dataframe\n",
    "optimization_df = pd.DataFrame()\n",
    "for i in optimization_df_parallel:\n",
    "    optimization_df = optimization_df.append(i)\n",
    "\n",
    "    optimization_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact = optimization_df.loc[optimization_df.model=='Naive weekly'].copy()#.join('tmp.y.loc[optimization_df.index]')\n",
    "fact.set_index(['ts','item_id'], inplace=True)\n",
    "fact['model'] = 'Fact'\n",
    "fact['value'] = data.reset_index().set_index(['date','item_id'])['sales']\n",
    "\n",
    "fact.reset_index(inplace=True)\n",
    "\n",
    "if len(optimization_df.loc[optimization_df.model=='Fact'])==0:\n",
    "    optimization_df = optimization_df.append(fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "alt.Chart(optimization_df.reset_index()).mark_line().encode(\n",
    "    x='ts:T',\n",
    "    y=alt.Y('value:Q',scale=alt.Scale(zero=True)),\n",
    "    color=alt.Color('prediction_date:N', legend=None),\n",
    "    row='model:N',\n",
    "    column='item_id:N'\n",
    ").properties(width=200, height=150)#.interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function for all item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimization_df.set_index(['item_id','prediction_date','ts'])\n",
    "from sklearn import metrics\n",
    "import math \n",
    "def get_item_rmse(item_id):\n",
    "    \n",
    "    res = pd.DataFrame(index=optimization_df.model.unique(), columns=['rmse'])\n",
    "    \n",
    "    df = optimization_df.loc[optimization_df.item_id==item_id].set_index(['prediction_date','ts'])\n",
    "        \n",
    "    for m in optimization_df.model.unique():\n",
    "        mse = metrics.mean_squared_error(df.loc[df.model=='Fact','value'],df.loc[df.model==m,'value'])        \n",
    "        res.loc[m,'rmse'] = math.sqrt(mse)\n",
    "    \n",
    "    res['rmse_normalized'] = res['rmse']/res.loc['Naive weekly','rmse']\n",
    "    res['item_id'] = item_id\n",
    "    res = res.drop('Fact').reset_index()\n",
    "    res.columns = ['model','rmse','rmse_normalized','item_id']\n",
    "    return res\n",
    "\n",
    "eval_df = pd.DataFrame()\n",
    "\n",
    "for item in top_items.values:\n",
    "    eval_df = eval_df.append(get_item_rmse(item), ignore_index=True)\n",
    "    \n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(eval_df).mark_circle().encode(\n",
    "    x=alt.X('rmse:Q',scale=alt.Scale(zero=False)),\n",
    "    y=alt.Y('rmse_normalized:Q',scale=alt.Scale(zero=False)),\n",
    "    color=alt.Color('model:N'),\n",
    "    column='item_id:N'\n",
    ").properties(width=100, height=150)#.interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(eval_df.groupby(['model']).sum()[['rmse_normalized','rmse']]/5).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart((eval_df.groupby(['model']).sum()[['rmse_normalized','rmse']]/5).reset_index()).mark_circle().encode(\n",
    "    x=alt.X('rmse:Q',scale=alt.Scale(zero=False)),\n",
    "    y=alt.Y('rmse_normalized:Q',scale=alt.Scale(zero=False)),\n",
    "    color=alt.Color('model:N'),\n",
    ").properties(width=200, height=250)#.interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with calculating actual daily revenue for each of the items (value * price) and checking total revenue for the set.\n",
    "Price for each item is fixed as average of all prices available in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_prices = data_subset.loc[data_subset.item_id.isin(top_items)].reset_index().groupby('item_id').mean()['sell_price']\n",
    "optimization_df.set_index(['item_id'], inplace=True)\n",
    "optimization_df['sell_price'] = items_prices\n",
    "optimization_df.reset_index(inplace=True)\n",
    "optimization_df['daily_revenue'] = optimization_df.value*optimization_df.sell_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_df.groupby(['model','prediction_date','ts']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''alt.Chart(optimization_df.groupby(['model','prediction_date','ts']).sum().reset_index()).mark_line().encode(\n",
    "    x='ts:T',\n",
    "    y=alt.Y('daily_revenue:Q',scale=alt.Scale(zero=True)),\n",
    "    color=alt.Color('prediction_date:N', legend=None),\n",
    "    row='model:N',\n",
    ").properties(width=400, height=150)#.interactive()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_revenue_bl = optimization_df.groupby(['model','prediction_date']).sum().daily_revenue.reset_index()\n",
    "weekly_revenue_bl = weekly_revenue_bl.pivot_table(columns='model',index='prediction_date', values='daily_revenue', aggfunc='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(weekly_revenue_bl).transform_fold(\n",
    "    weekly_revenue_bl.columns.values,\n",
    "    #['Fact'],\n",
    "    as_=['Model', 'Measurement']\n",
    ").mark_bar(\n",
    "    opacity=0.5,\n",
    "    binSpacing=0\n",
    ").encode(\n",
    "    alt.X('Measurement:Q', bin=alt.Bin(maxbins=50)),\n",
    "    alt.Y('count()', stack=None),\n",
    "    alt.Color('Model:N')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization target:\n",
    "* Every week we make predictions for each item and calculate total revenue for the week\n",
    "* If the revenue is lower than $19k, we implement a discount to increase sales. Discounts are implemented for each induvidual product\n",
    "* Effect of a discount is preset - we get 1.5% increase in sales for each 1% discount\n",
    "* Limitations: one product can't be on discount for more than 2 consecutive weeks; Amount of discount shouldn't be more than 50%; limit on max amount of items sold is 150% of maximum sold during the period we are looking at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sales = optimization_df.loc[optimization_df.model=='Fact'].groupby(['item_id','prediction_date']).sum().reset_index()\n",
    "\n",
    "max_capacity = (weekly_sales.groupby('item_id').max()['value']*1.5).apply(math.ceil)\n",
    "max_capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate = optimization_df.loc[(optimization_df.prediction_date=='2011-11-30')&(optimization_df.model=='Naive weekly')]\n",
    "LOWER_SALES_LIMIT = 30000\n",
    "MAX_CAPACITY = (optimization_df.loc[optimization_df.model=='Fact'].groupby(['item_id','prediction_date']).sum().reset_index().groupby('item_id').max()['value']*1.5).apply(math.ceil)\n",
    "\n",
    "#discounts = ng.p.Array(shape=(len(top_items),)).set_bounds(lower=0, upper=.5,)\n",
    "discounts = ng.p.Choice(range(0,60,10),repetitions=len(top_items))\n",
    "last_2w_discounts = [0,1,0,1,0]\n",
    "instru = ng.p.Instrumentation(discounts,MAX_CAPACITY,last_2w_discounts)\n",
    "optimizer = ng.optimizers.CMA(parametrization=instru, budget=1000)\n",
    "\n",
    "\n",
    "def cost_function(solution,capacity_limits,last_2w_discounts):\n",
    "    capacity_penalty = 1\n",
    "    w3_discount_penalty = 1\n",
    "    penalty_lower_than_forecast = 1\n",
    "    \n",
    "    solution = np.array(solution)/100\n",
    "    candidate = optimization_df.loc[(optimization_df.prediction_date=='2011-11-30')&(optimization_df.model=='Naive weekly')]\n",
    "    weekly_sales = candidate.loc[candidate.model=='Naive weekly'].groupby(['item_id','prediction_date']).sum().reset_index()\n",
    "    max_capacity = (weekly_sales.groupby('item_id').max()['value']*1.5).apply(math.ceil)\n",
    "    value_increase = pd.DataFrame(index=items_prices.index, data=solution*1.5, columns=['value_increase']) #1.5% increase in sales for each 1% increase in discount\n",
    "    new_price = items_prices*(1 - solution)\n",
    "    \n",
    "    \n",
    "    candidate = candidate.join(new_price,on='item_id', how='left', rsuffix = '_w_disount')\n",
    "    candidate = candidate.join(value_increase,on='item_id', how='left')\n",
    "    candidate['value_w_disount'] = candidate.value + candidate.value_increase*candidate.value\n",
    "    candidate['daily_revenue_w_disount'] = candidate['value_w_disount']*candidate['sell_price_w_disount']\n",
    "    \n",
    "    #penalty for exceeding limit on max amount of items sold (110% of maximum sold during the period we are looking at)\n",
    "    if any(candidate.groupby('item_id').sum()['value_w_disount']>MAX_CAPACITY):\n",
    "        capacity_penalty = 5000\n",
    "        \n",
    "\n",
    "    #penalty for having an item on sale for more than 2 weeks    \n",
    "    if np.count_nonzero(solution*last_2w_discounts)>0:\n",
    "        w3_discount_penalty = 5000*np.count_nonzero(solution*last_2w_discounts)\n",
    "        #print('w3_discount_penalty')\n",
    "        \n",
    "    \n",
    "    #penalty if daily_revenue_w_disount is lower than forecasted revenue without it\n",
    "    if sum(candidate['daily_revenue_w_disount'])<sum(candidate['daily_revenue']):\n",
    "        penalty_lower_than_forecast = 10000\n",
    "        #print('penalty_lower_than_forecast')        \n",
    "            \n",
    "    total_cost = sum(candidate['daily_revenue_w_disount'])+capacity_penalty+w3_discount_penalty+penalty_lower_than_forecast\n",
    "    \n",
    "    #if (capacity_penalty + w3_discount_penalty + penalty_lower_than_forecast) == 3:\n",
    "        #print('found a non-penalty solution!', solution,total_cost)\n",
    "    \n",
    "    \n",
    "    return total_cost\n",
    "\n",
    "recommendation = optimizer.minimize(cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*(cost_function(np.array([30, 0, 10, 0, 10]),MAX_CAPACITY,last_2w_discounts)/sum(candidate['daily_revenue'])-1),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vof import optimizer\n",
    "\n",
    "class OptimalDecisionMakersSales(optimizer.OptimalDecisionMakers):\n",
    "    \n",
    "\n",
    "    def __init__(self, data, target):\n",
    "        super().__init__(data, target)\n",
    "        self.EVALUATE = False\n",
    "        self.TIME_HORIZON = 7\n",
    "        self.DECISION_TIMESTEP = 7\n",
    "        self.MAX_CAPACITY = (data.loc[data.model=='Fact'].groupby(['item_id','prediction_date']).sum().reset_index().groupby('item_id').max()['value']*1.5).apply(math.ceil)\n",
    "        #self.LOWER_SALES_LIMIT = 19000\n",
    "        self.BASIC_PRICES = items_prices\n",
    "        self.LAST2W_DISCOUNTS = [0,0,0,0,0]\n",
    "            \n",
    "    def cost_function(self,solution,capacity_limits,last_2w_discounts):\n",
    "        capacity_penalty = 1\n",
    "        w3_discount_penalty = 1\n",
    "        penalty_lower_than_forecast = 1\n",
    "\n",
    "        solution = np.array(solution)/100\n",
    "        #candidate = optimization_df.loc[(optimization_df.prediction_date=='2011-11-30')&(optimization_df.model=='Naive weekly')]\n",
    "        \n",
    "        weekly_sales = self.candidate.loc[self.candidate.model=='Naive weekly'].groupby(['item_id','prediction_date']).sum().reset_index()\n",
    "        max_capacity = (weekly_sales.groupby('item_id').max()['value']*1.5).apply(math.ceil)\n",
    "        value_increase = pd.DataFrame(index=items_prices.index, data=solution*1.5, columns=['value_increase']) #1.5% increase in sales for each 1% increase in discount\n",
    "        new_price = items_prices*(1 - solution)\n",
    "        \n",
    "        #print(self.candidate)\n",
    "        self.candidate.set_index('item_id', inplace=True)\n",
    "        self.candidate['sell_price_w_disount'] = new_price\n",
    "        self.candidate['value_increase'] = value_increase\n",
    "        #self.candidate = self.candidate.join(new_price,on='item_id', how='left', rsuffix = '_w_disount')\n",
    "        #self.candidate = self.candidate.join(value_increase,on='item_id', how='left')\n",
    "\n",
    "        self.candidate['value_w_disount'] = self.candidate.value + self.candidate.value_increase*self.candidate.value\n",
    "        self.candidate['daily_revenue_w_disount'] = self.candidate['value_w_disount']*self.candidate['sell_price_w_disount']\n",
    "        \n",
    "        self.candidate.reset_index(inplace=True)\n",
    "        #penalty for exceeding limit on max amount of items sold (110% of maximum sold during the period we are looking at)\n",
    "        if any(self.candidate.groupby('item_id').sum()['value_w_disount']>MAX_CAPACITY):\n",
    "            capacity_penalty = 5000\n",
    "\n",
    "\n",
    "        #penalty for having an item on sale for more than 2 weeks    \n",
    "        if np.count_nonzero(solution*last_2w_discounts)>0:\n",
    "            w3_discount_penalty = 5000*np.count_nonzero(solution*last_2w_discounts)\n",
    "            #print('w3_discount_penalty')\n",
    "\n",
    "\n",
    "        #penalty if daily_revenue_w_disount is lower than forecasted revenue without it\n",
    "        if sum(self.candidate['daily_revenue_w_disount'])<sum(self.candidate['daily_revenue']):\n",
    "            penalty_lower_than_forecast = 10000\n",
    "            #print('penalty_lower_than_forecast')        \n",
    "\n",
    "        total_cost = sum(self.candidate['daily_revenue_w_disount'])+capacity_penalty+w3_discount_penalty+penalty_lower_than_forecast\n",
    "\n",
    "        #if (capacity_penalty + w3_discount_penalty + penalty_lower_than_forecast) == 3:\n",
    "            #print('found a non-penalty solution!', solution,total_cost)\n",
    "\n",
    "\n",
    "        return total_cost\n",
    "\n",
    "    \n",
    "    def calculate_optimal_decision(self, model):\n",
    "        \n",
    "        print(model)\n",
    "\n",
    "        total_steps = self.data.prediction_date.unique()\n",
    "\n",
    "        #recommendation = np.array([])\n",
    "        recommendation = pd.DataFrame()\n",
    "        last_2w_discounts = self.LAST2W_DISCOUNTS\n",
    "        #battery_ch = self.BATTERY_CHARGE\n",
    "\n",
    "        for step in total_steps:\n",
    "            print(step)\n",
    "            \n",
    "            discounts = ng.p.Choice(range(0,60,10),repetitions=len(top_items))\n",
    "            #print('define instru')\n",
    "            instru = ng.p.Instrumentation(discounts,self.MAX_CAPACITY,last_2w_discounts)\n",
    "            optimizer = ng.optimizers.CMA(parametrization=instru, budget=100)\n",
    "\n",
    "            try:\n",
    "                \n",
    "                self.candidate = self.data.loc[(self.data.prediction_date==step)&(self.data.model==model)]\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                print(self.data.loc[(self.data.prediction_date==step)&(self.data.model==model),'value'])\n",
    "\n",
    "            #print('step_recommendation')\n",
    "            \n",
    "            step_recommendation = optimizer.minimize(self.cost_function)[0][0].value[0:self.DECISION_TIMESTEP]\n",
    "            step_recommendation = np.array(step_recommendation)/100\n",
    "            value_increase = pd.DataFrame(index=items_prices.index, data=step_recommendation*1.5, columns=['value_increase'])\n",
    "            #print(value_increase)\n",
    "            \n",
    "            self.candidate.set_index('item_id', inplace=True)\n",
    "            \n",
    "            self.candidate['disount'] = items_prices*(1 - step_recommendation)\n",
    "            #print(step_recommendation)\n",
    "            #print(items_prices*(1 - step_recommendation))\n",
    "            self.candidate['sell_price_w_disount'] = self.candidate['sell_price']*self.candidate['disount']\n",
    "            self.candidate['value_increase'] = value_increase\n",
    "            self.candidate['value_w_disount'] = self.candidate.value + self.candidate.value_increase*self.candidate.value\n",
    "            self.candidate['daily_revenue_w_disount'] = self.candidate['value_w_disount']*self.candidate['sell_price_w_disount']\n",
    "            #print(self.candidate.head(5))\n",
    "            self.candidate.reset_index(inplace=True)\n",
    "            #because step_recommendation returns a tuple, we need to access element with index [0][0]:           \n",
    "            #recommendation = np.append(recommendation,step_recommendation[0][0].value[0:self.DECISION_TIMESTEP])\n",
    "            \n",
    "            #print('trying to append')\n",
    "            recommendation = recommendation.append(pd.DataFrame(self.candidate.iloc[0:self.DECISION_TIMESTEP]))\n",
    "            \n",
    "            last_2w_discounts = sum(step_recommendation)\n",
    "        \n",
    "        return recommendation#pd.Series(name=model, data=recommendation)\n",
    "    \n",
    "\n",
    "    def select_value_optimal_model(self,models,return_predictions=False):\n",
    "        \n",
    "        random.seed(42)\n",
    "                \n",
    "        self.optimal_decision_result = (Parallel(n_jobs=-1, verbose = 1000)(delayed(self.calculate_optimal_decision)(model) for model in models))\n",
    "\n",
    "        return self.optimal_decision_result\n",
    "    \n",
    "    def evaluate(self):\n",
    "        return evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odm = OptimalDecisionMakersSales(optimization_df,'Fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sol = odm.select_value_optimal_model()\n",
    "sol = odm.select_value_optimal_model(models=optimization_df.model.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input:\n",
    "\n",
    "    * prediction for item_id (array)\n",
    "    * prices for each item - we going to control these\n",
    "    * cost function: total weekly revenue\n",
    "    * constraints: number of items that can be sold, 110% of max weekly sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m61"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
